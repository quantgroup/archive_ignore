{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from numpy import mean, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE n_rows TO WORK WITH FULL DATASET\n",
    "\n",
    "# variable inputs\n",
    "lag_tol = 90 # 90 days is the minimum lag tolerance\n",
    "n_rows = 1000 # want to perform large calcs on a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_data = pd.read_csv(\"C:/model_data/estimate_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the initial index column\n",
    "est_data.drop('Unnamed: 0', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_id and currency -> category form\n",
    "est_data['source_id'] = est_data['source_id'].astype('category')\n",
    "est_data['currency'] = est_data['currency'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to convert estimate and period dates to dates\n",
    "est_data['period_date'] = pd.to_datetime(est_data['period_date'])\n",
    "est_data['estimate_date'] = pd.to_datetime(est_data['estimate_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_est = (est_data\n",
    "               .groupby(['security_id', 'broker_id', 'period_date'])\n",
    "               ['estimate_date']\n",
    "               .max()\n",
    "          )\n",
    "\n",
    "min_est = (est_data\n",
    "               .groupby(['security_id', 'broker_id', 'period_date'])\n",
    "               ['estimate_date']\n",
    "               .min()\n",
    "          )\n",
    "\n",
    "lag_bool = (max_est - min_est).dt.days > lag_tol\n",
    "\n",
    "est_data = (\n",
    "    est_data\n",
    "    .join(lag_bool, \n",
    "          on = ['security_id', 'broker_id', 'period_date'],\n",
    "          rsuffix = '_in')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_data = est_data[est_data.estimate_date_in == True]\n",
    "est_data.drop('estimate_date_in', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_data.sort_values(['security_id', 'broker_id', 'period_date', 'estimate_date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just makes the time series daily between revision dates\n",
    "est_data_daily = (\n",
    "    est_data.head(n_rows)\n",
    "    .set_index(['security_id', 'broker_id', 'period_date'])\n",
    "    .groupby(['security_id', 'broker_id', 'period_date'])\n",
    "    .apply(\n",
    "        lambda df: df.drop_duplicates('estimate_date')\n",
    "                     .set_index('estimate_date')\n",
    "                     .resample('D')\n",
    "                     .ffill()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_df(df, lag_tol):\n",
    "    \n",
    "    # shift data\n",
    "    shifted_df = df.shift(periods = -lag_tol)\n",
    "    \n",
    "    # assign na\n",
    "    shifted_df.value.fillna(est_data_daily.value[-1], inplace = True)\n",
    "    shifted_df.currency.fillna(est_data_daily.currency[-1], inplace = True)\n",
    "    shifted_df.source_id.fillna(est_data_daily.source_id[-1], inplace = True)\n",
    "    \n",
    "    # shift date\n",
    "    shifted_df.reset_index(['security_id', 'broker_id', 'period_date', 'estimate_date'], \n",
    "                           inplace = True)\n",
    "    shifted_df['estimate_date'] = shifted_df['estimate_date'] + datetime.timedelta(days = 90)\n",
    "    \n",
    "    # append out-shifted portion\n",
    "    df.reset_index(['security_id', 'broker_id', 'period_date', 'estimate_date'], \n",
    "                   inplace = True)\n",
    "    df_append = df[df.estimate_date < shifted_df.estimate_date.min()]\n",
    "    shift_full = (pd.concat([df_append, shifted_df])\n",
    "                      .set_index(['security_id', 'broker_id', 'period_date'])\n",
    "                 )\n",
    "    return shift_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_changes(df_changes, lag_tol):\n",
    "    \n",
    "    # calculating changes\n",
    "    changes = (df_changes['value'][lag_tol:]\n",
    "                   .subtract(df_changes['value'][:(-lag_tol)])\n",
    "              )\n",
    "    changes.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # adding back changes that were 0\n",
    "    changes_full = (changes.shift(lag_tol, fill_value = 0)[:lag_tol]\n",
    "                       .append(changes)\n",
    "                       .rename('value_changes')\n",
    "                   )\n",
    "    changes_full.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # adding the vector to the df\n",
    "    df_changes.reset_index(['security_id', 'broker_id', 'period_date'], \n",
    "                         inplace = True)\n",
    "    df_changes.insert(5, \n",
    "                    'changes', \n",
    "                    changes_full, \n",
    "                    allow_duplicates = True)\n",
    "    \n",
    "    # add true false columns to sum\n",
    "    df_changes = df_changes.assign(up = (df_shift.changes > 0))\n",
    "    df_changes = df_changes.assign(down = (df_shift.changes < 0))\n",
    "    \n",
    "    return df_changes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through and storing results\n",
    "init = False\n",
    "for name_gp, df_gp in est_data_daily.groupby(['security_id', 'broker_id', 'period_date']):\n",
    "    df_shift = shift_df(df_gp, lag_tol)\n",
    "    df_w_changes = add_changes(df_shift, lag_tol)\n",
    "    if init == False:\n",
    "        df_calc = df_w_changes\n",
    "        init = True\n",
    "    else:\n",
    "        df_calc = pd.concat([df_calc, df_w_changes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the laplace factor calculation accross brokers\n",
    "# need to collapse broker_id col here... then the rest will run smoothly\n",
    "df_factor = ((df_calc\n",
    "                 .groupby(['security_id', 'period_date', 'estimate_date', 'source_id'], as_index = False)\n",
    "                 .apply(lambda df_ind: (sum(df_ind.up) - sum(df_ind.down))/(sum(df_ind.up) + sum(df_ind.down) + 2))\n",
    "            )\n",
    "            .rename('factor_value')\n",
    "            .reset_index(['security_id', 'period_date', 'estimate_date', 'source_id'])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting the factor based on optimal forecast period\n",
    "df_factor['scalar_weight'] = (stats.norm(550, 310)\n",
    "                              .pdf((df_factor.period_date - df_factor.estimate_date).dt.days)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = (df_factor.groupby(['security_id', 'estimate_date', 'source_id'])\n",
    "                .apply(lambda df: dot(df.factor_value, df.scalar_weight)/sum(df.scalar_weight))\n",
    "            ).rename('factor_value').reset_index(['security_id', 'estimate_date', 'source_id'])\n",
    "df_out['factor_id'] = 'lap_breadth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.reindex(columns = ['factor_id', 'estimate_date', 'security_id', 'factor_value', 'source_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.rename(columns = {'estimate_date':'date', 'factor_value':'value'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6548.000000\n",
       "mean       -0.002384\n",
       "std         0.344256\n",
       "min        -0.855590\n",
       "25%        -0.198952\n",
       "50%         0.000000\n",
       "75%         0.300129\n",
       "max         0.819532\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.value.describe() # a five number summary of the laplace breadth factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('factor_out.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitdabd9e4a2d5d4040bc1170cd428d0e0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
