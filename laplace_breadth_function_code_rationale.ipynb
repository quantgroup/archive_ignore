{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is estimate breadth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textrm{estimate breadth} = \\frac{\\textrm{estimate upgrades - estimate downgrades}}{\\textrm{total number of estimates}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it not sufficient on its own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of the following 3 Amazon review histories for three sellers:\n",
    "    * 2/2 positive reviews. 100 % positive\n",
    "    * 9/10 positive reviews. 90 % positive \n",
    "    * 88/100 postiive reviews. 88% positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not all of the review histories are equal, and you would probably feel that seller number 3 is going to give you the best experience. But how do we incorporate this in mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Bayesian prior obviously... With a uniform prior (i.e. one additional negative and positive review), we can retrieve a postierior that is closer to our real beliefs. Taking the first seller as an example:\n",
    "\n",
    "$$\\textrm{(Prior 1)     Bin} \\sim (\\hat{p} = \\frac{1}{2}, n = 2)$$\n",
    "$$\\textrm{(Prior 2)      Bin} \\sim (\\hat{p} = \\frac{2}{2}, n = 2)$$\n",
    "\n",
    "$$\\textrm{(Posterior)      Beta} \\sim (\\hat{\\alpha} = \\frac{3}{4}, \\hat{\\beta} = \\frac{1}{4})$$\n",
    "\n",
    "$$\\hat{\\mathbb{E}}[\\textrm{Success}_{\\textrm{post}}] = \\frac{\\frac{3}{4} + \\frac{1}{4}}{\\frac{3}{4}} = 75\\%$$\n",
    "\n",
    "Whereas it is $87.2\\%$ for seller 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats pretty neat. The true optimal solution is actually the center of mass for the beta PDF... but we do not want to do any hectic maths in the calculation, so this quick and dirty alternative will work great for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace breadth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose we call it Laplace breadth because although pastor Bayes discovered the formula, he was a layman, and his work gained no notice until Laplace dragged it kicking and screaming into 20th century maths along with the rest of probability theory. And Laplace even proposed a similar thought experiment in his own writings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revised estimate breadth formula is:\n",
    "\n",
    "$$\\textrm{laplace breadth} = \\frac{(\\textrm{estimate upgrades} + 1) - (\\textrm{estimate downgrades} + 1)}{\\textrm{total estimates} + 2}$$\n",
    "\n",
    "$$\\textrm{laplace breadth} = \\frac{(\\textrm{estimate upgrades}) - (\\textrm{estimate downgrades})}{\\textrm{total estimates} + 2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitdabd9e4a2d5d4040bc1170cd428d0e0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
